{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling and Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are currently 120542 points in our data set.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Alex Brosas another idiot #ALDUBKSGoesToUS  ht...</td>\n",
       "      <td>alex brosas idiot aldubksgoestous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @ItIzBiz: as Nancy Reagan would say, 'just ...</td>\n",
       "      <td>nancy reagan fucking like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @MailOnline: The Nazi death gas so horrific...</td>\n",
       "      <td>nazi death gas horrific hitler fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I hate er chase because if the Bitch that work...</td>\n",
       "      <td>hate er chase bitch work literally evil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @chevleia: don't hmu when u get tired of ur...</td>\n",
       "      <td>hmu tired ur bore hoe ur bore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet  \\\n",
       "0      0  Alex Brosas another idiot #ALDUBKSGoesToUS  ht...   \n",
       "1      0  RT @ItIzBiz: as Nancy Reagan would say, 'just ...   \n",
       "2      0  RT @MailOnline: The Nazi death gas so horrific...   \n",
       "3      1  I hate er chase because if the Bitch that work...   \n",
       "4      0  RT @chevleia: don't hmu when u get tired of ur...   \n",
       "\n",
       "                               clean_tweet  \n",
       "0        alex brosas idiot aldubksgoestous  \n",
       "1                nancy reagan fucking like  \n",
       "2      nazi death gas horrific hitler fear  \n",
       "3  hate er chase bitch work literally evil  \n",
       "4            hmu tired ur bore hoe ur bore  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv('data/clean_data.csv', lineterminator='\\n')\n",
    "print(f'There are currently {len(df)} points in our data set.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our subsample has 30135 data points.\n",
      "The training split of the subsample has 24108 data points.\n",
      "The test split of the subsample has 6027 data points.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features and target label \n",
    "X = df.clean_tweet\n",
    "y = df.label\n",
    "\n",
    "# Get smaller subset of the data\n",
    "# NOTE: Since the classes are imbalanced, we use a stratified random split \n",
    "X_small, X_big, y_small, y_big = train_test_split(X, y, stratify=y, test_size=0.75)\n",
    "print(f'Our subsample has {len(X_small)} data points.')\n",
    "\n",
    "# Split subsample into training and test sets. \n",
    "# Once again, we use a stratified split. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, stratify=y_small, test_size=0.2)\n",
    "print(f'The training split of the subsample has {len(X_train)} data points.')\n",
    "print(f'The test split of the subsample has {len(X_test)} data points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1323: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "{'bow__ngram_range': (1, 2), 'model__C': 0.01, 'model__class_weight': None, 'model__penalty': 'none', 'tfidf__norm': 'l1', 'tfidf__use_idf': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96      5613\n",
      "           1       0.50      0.32      0.39       414\n",
      "\n",
      "    accuracy                           0.93      6027\n",
      "   macro avg       0.73      0.65      0.68      6027\n",
      "weighted avg       0.92      0.93      0.92      6027\n",
      "\n",
      "\n",
      "MultinomialNB\n",
      "{'bow__ngram_range': (1, 2), 'model__alpha': 0.5, 'model__fit_prior': False, 'tfidf__norm': 'l2', 'tfidf__use_idf': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      5613\n",
      "           1       0.42      0.38      0.40       414\n",
      "\n",
      "    accuracy                           0.92      6027\n",
      "   macro avg       0.69      0.67      0.68      6027\n",
      "weighted avg       0.92      0.92      0.92      6027\n",
      "\n",
      "\n",
      "RandomForestClassifier\n",
      "{'bow__ngram_range': (1, 1), 'model__ccp_alpha': 0, 'model__class_weight': 'balanced', 'model__n_estimators': 100, 'tfidf__norm': 'l1', 'tfidf__use_idf': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      5613\n",
      "           1       0.71      0.21      0.32       414\n",
      "\n",
      "    accuracy                           0.94      6027\n",
      "   macro avg       0.83      0.60      0.65      6027\n",
      "weighted avg       0.93      0.94      0.92      6027\n",
      "\n",
      "\n",
      "XGBClassifier\n",
      "{'bow__ngram_range': (1, 1), 'model__eta': 0.6, 'model__max_depth': 10, 'model__scale_pos_weight': 10, 'tfidf__norm': 'l1', 'tfidf__use_idf': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      5613\n",
      "           1       0.41      0.49      0.44       414\n",
      "\n",
      "    accuracy                           0.92      6027\n",
      "   macro avg       0.68      0.72      0.70      6027\n",
      "weighted avg       0.92      0.92      0.92      6027\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of potential models to try out with different parameters\n",
    "model_list = [\n",
    "    (LogisticRegression(solver='saga'), {\n",
    "        'model__penalty' : ['l1', 'l2', 'none'],\n",
    "        'model__C' : [0.01, 0.1, 1],\n",
    "        'model__class_weight' : ['balanced', None],      \n",
    "    }),\n",
    "    (MultinomialNB(), {\n",
    "        'model__fit_prior': [True, False],\n",
    "        'model__alpha' : [0, 0.01, 0.1, 0.5, 0.8, 1]\n",
    "    }), \n",
    "    (RandomForestClassifier(), {\n",
    "        'model__class_weight' : ['balanced', None],\n",
    "        'model__n_estimators' : [10, 100, 1000],\n",
    "        'model__ccp_alpha' : [0, 0.01, 0.1]\n",
    "    }), \n",
    "    (XGBClassifier(eval_metric='logloss', use_label_encoder=False), {\n",
    "        'model__scale_pos_weight' : [1, 10],\n",
    "        'model__max_depth' : [2, 6, 10],\n",
    "        'model__eta' : [0.01, 0.3, 0.6]\n",
    "    })\n",
    "]\n",
    "\n",
    "# Do a grid search cross validation over the parameter grid for each model, and print the results \n",
    "for model in model_list:\n",
    "    \n",
    "    pipe = Pipeline([('bow', CountVectorizer(min_df=2)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('model', model[0])])\n",
    "    param_grid = {\n",
    "        'bow__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "    }\n",
    "    param_grid.update(model[1])\n",
    "    \n",
    "    clf = GridSearchCV(pipe, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(str(model[0]).split('(')[0])\n",
    "    print(clf.best_params_)\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
